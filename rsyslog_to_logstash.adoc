= Piping logs to Logstash using Rsyslog
David Welman
2019-06-01

== Prelude
The typical method for getting trailing log files into Logstash is using the ELK stack tool Filebeat. It was found that Filebeat could not be installed on z-linux hence the need for an alternative.

=== rsyslog
rsyslog is a logging utility found on most linux distributions, including z-linux, that can monitor files and directories for changes and then log those using a format called 'syslog' hence the name. The way rsyslog is being used in this situation is simple:

 - rsyslog has been configured to be notified of updates of a particular file found under "/var/log".
 - It has been configured to send said updates of that file to a particular server address over a specific port using TCP.

The following line is required to enable rsyslog to monitor external files for updates:

./etc/rsyslog.conf
---------------
module(load="imfile" mode="inotify")
---------------

"inotify" is always preferable to polling and should always be used unless bugs are encountered with it.

---------------
module(load="imfile" mode="polling" PollingInterval=10)
---------------

As mentioned "inotify" is preferable, as PollingInterval is obsolete and legacy.

Next, create a config file for the specific file to be monitored, this will also contain the config information necessary to send the tailed logs to the Logstash server. 

../rsyslog/tcp-out.conf
--------------
include::rsyslog/tcp-out.conf[]
--------------

This file should be saved under "/etc/rsyslog.d/tcp-out.conf"

Once the following config files have been updated and created, the following command can be run to ensure that the added config is valid and is free of syntax errors:
--------
    rsyslogd -N 1
--------

Once the newly added changes have been verified, rsyslog can be restarted through the following command:
-----
    sudo service rsyslog restart
-----

=== Logstash
Logstash is a part of the ELK stack that acts as log storage for Elastic Search. The following will be configuration steps to allow it to receive messages on a specific port over TCP, as well as a Grok pattern to transform incoming logs from Websphere.

In the folder that the pipelines are found for Logstash, for example "/pipeline" on a docker container, add a new config file with the following config details in it:

-----
    input {
        tcp {
            port => XXXX
        }
    }
-----

Set the port for the TCP input to whatever port is being used as per setup, the default is '5044'.

This is the Grok pattern that can be used to configure a log from IBM Websphere:

-----
    filter {
        grok {
            match => { "message" => "\[%{DATE:date} %{TIME:time} %{WORD:timezone}\] %{BASE16NUM:address} %{WORD:component} %{WORD:loglevel} %{DATA:classname} %{WORD:request} %{GREEDYDATA:errormessage}" }
            overwrite => ["message"]
        }
    }
-----

This filter can be added to the above Logstash config file in the following format:

-----

    input {
        ...
    }
    filter {
        ...
    }
    output {
        ...
    }
-----

This brings us to the final complete file:

../pipeline/logstash.conf
-------
include::logstash/logstash.conf[]
-------

As for output the current setup will print straight to stdout which allows for debugging to ensure everything is working, this can later be updated to pipe the logs to Elastic or configured for any other purposes.

=== Testing

A complete test environment can be set up with docker-compose, and the entire stack will work by piping logs into Logstash. Using the provided docker-compose file:

../docker-compose.yml
[source, yaml]
-----
include::docker-compose.yml[]
-----

The way this stack is composed it will work as configured, with logstash talking to elasticsearch talking to kibana. All that needs to be done on the machine scanning and sending the logs is to replace the IP in the above rsyslog config file with the IP address of the logstash container. Once this has been configured, piping log lines to the monitored log file will kick off the process.

To ensure logs are being picked up and sent through to logstash and elastic, the Kibana dashboard can be viewed on the port as configured in the docker-compose.yml